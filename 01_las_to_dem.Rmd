---
title: "01_las_to_dem"
author: "Matthew Coghill"
date: "11/30/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The primary purpose of this file will be to process the downloaded LAS file into raster products (DEM and DSM). First, core libraries need to be loaded. We will also define our output map resolution here in meters.

```{r Load Libraries, include=FALSE}

library(data.table)
library(tidyverse)
library(lidR)
library(future)
library(terra)

# Define desired resolution of the output map in meters
map_res <- 0.05
chm_fast <- TRUE

# Create text string for file outputs
res_txt <- if(map_res >= 1) {
  paste0("_", map_res, "m")
} else {
  paste0("_", map_res * 100, "cm")
}

```

The first thing that needs to happen is tiling. The LAS file that I received was very large and difficult to work with on most machines. Tiling gets around that by breaking the large LAS file into many (36) smaller tiles. This helps to manage memory and increases efficiency since we can get machine cores working on each tile. If your machine has 16 cores, you can get a maximum of 16 tiles loaded and processed at once, but make sure to work within the limits of your machine since individual user experience will vary.

Tiling is able to happen by defining the chunk option `opt_chunk_size`, so it can happen as part of any LAS function. In this instance, I am going to reclassify ground points after a 250x250 m chunk is loaded. We will continue that in the next chunk, but in the chunk below we are just creating the directories for all of the outputs of this file.

```{r Tiling}

# Define directories
dl_dir <- file.path("./01_download_raw")
tile_dir <- file.path("./02_tile")
dem_dir <- file.path("./03_dem_tiles")
dsm_dir <- file.path("./04_dsm_tiles")
norm_dir <- file.path("./05_normalized")
chm_dir <- file.path("./06_chm_tiles")
prod_dir <- file.path("./outputs")

dir.create(tile_dir, showWarnings = FALSE)
dir.create(dem_dir, showWarnings = FALSE)
dir.create(dsm_dir, showWarnings = FALSE)
dir.create(norm_dir, showWarnings = FALSE)
dir.create(chm_dir, showWarnings = FALSE)
dir.create(prod_dir, showWarnings = FALSE)

```

The original LAS file came with some amount of classification completed, but return numbers were a little bit off. Proper ground point classification uses the last returns from a given pulse to classify those points as "ground", but the LAS file that was generated had the "NumberOfReturns" value set to 4 for each point. This is incorrect: the number of returns should be the maximum value of the "ReturnNumber" for a given pulse, thus this requires that the LAS files be worked on:

1. Retrieve a pulse ID for each point
2. Group data by pulse ID
3. Change the value in the "NumberOfReturns" column to be the maximum "ReturnNumber" in the data grouping
4. Perform ground point classification using the cloth simulated filter (CSF) algorithm

In doing so, ground point classification can be carried out with extreme accuracy without having to select different batches of points by classification numbers to operate on. Additionally, point indexation is supposed to assist in increasing processing time which is accomplished by creating .lax files. This step is completed by defining the output option "index" and changing it to `TRUE`. Finally, parallelizing is a bit of a mystery, but I found that using 4 for the lidR threads and 3 for the future cores seemed to do the trick for optimizing a timely delivery of products.

```{r Reclassify ground points}

# If a new boundary is being made, delete the old one
# file.remove(file.path(prod_dir, "shape_raw.gpkg"))

reclassify_ground <- function(las, update_bound = FALSE) {
  las <- readLAS(las)
  if (is.empty(las)) return(NULL)
  las <- retrieve_pulses(las)
  las@data <- las@data[, NumberOfReturns:=max(ReturnNumber), by=pulseID]
  las <- classify_ground(las, algorithm = csf(sloop_smooth = TRUE), last_returns = TRUE)
  if(update_bound | !file.exists(file.path(prod_dir, "shape_raw.gpkg"))) {
    hull <- concaveman(las$X, las$Y, concavity = 1, length_threshold = 5)
    hull <- st_sfc(st_polygon(list(as.matrix(hull))), crs = wkt(las))
    st_write(hull, file.path(prod_dir, "shape_raw.gpkg"), append = TRUE, quiet = TRUE)
  }
  las <- filter_poi(las, buffer == 0)
  return(las)
}

# Define catalog options
las_file <- readUAVLAScatalog(dl_dir)
opt_chunk_buffer(las_file) <- 12.5
opt_chunk_size(las_file) <- 250
opt_output_files(las_file) <- file.path(tile_dir, "{XLEFT}_{YBOTTOM}")
opt_chunk_alignment(las_file) <- c(
  plyr::round_any(xmin(las_file), 10, floor),
  plyr::round_any(ymin(las_file), 10, floor))
las_file@output_options$drivers$LAS$param$index <- TRUE

# Run the function
set_lidr_threads(4)
plan(multisession(workers = 3))
ctg <- catalog_apply(las_file, reclassify_ground, .options = list(automerge = TRUE))

# Load geopackage in and merge each shape into one
ctg_shp <- st_read(file.path(prod_dir, "shape_raw.gpkg"), quiet = TRUE) %>% 
  st_union()
st_write(ctg_shp, file.path(prod_dir, "shape_merged.gpkg"), quiet = TRUE)
ctg_shp <- vect(ctg_shp)

```

With the properly classified LAS tiles, we can now move on to DEM generation. There are a few different ways to go about this, but I chose using the TIN algorithm since it is commonly used and the default options are good for a wide variety of projects, so no algorithm tuning is necessary. The grid_terrain function loads surrounding tiles into memory when a given tile is loaded, so expect a large amount of memory to be used when generating the DEM. I ran into errors initially because of this, so I reduced the amount of cores allowed for processing, adjust to fit your machines needs.

I chose to generate intermediate .tif files here since my machine crashed part way through, though this is optional. Temporary files can also be made without defining output locations for those, though user beware: if R crashes, this process needs to be restarted from the beginning.

```{r DEM generation}

# Set catalog options
ctg <- readUAVLAScatalog(tile_dir, filter = "-keep_class 2 9")
opt_chunk_buffer(ctg) <- 12.5
opt_output_files(ctg) <- file.path(dem_dir, "{*}_dem")
ctg@output_options$drivers$Raster$param$overwrite <- TRUE

# Generate DEM
set_lidr_threads(4)
plan(multisession(workers = 3))
dem <- rast(grid_terrain(ctg, res = map_res, algorithm = tin(), Wdegenerated = FALSE))

# Write full raster to output folder
plan(sequential)
dem <- mask(dem, ctg_shp, filename = file.path(prod_dir, paste0("dem", res_txt, ".tif")),
            overwrite = TRUE)

```

The same ideas used in DEM generation are also used for DSM generation. Again, temporary files can be made instead of the permanent .tif tiles, but use at your discretion.

```{r DSM generation}

# Set catalog options
ctg <- readUAVLAScatalog(tile_dir)
opt_chunk_buffer(ctg) <- 12.5
opt_output_files(ctg) <- file.path(dsm_dir, "{*}_dsm")
ctg@output_options$drivers$Raster$param$overwrite <- TRUE

# Generate DSM
set_lidr_threads(4)
plan(multisession(workers = 3))
dsm <- grid_canopy(ctg, map_res, dsmtin())

# Write full raster to output folder; DSM has holes
plan(sequential)
writeRaster(dsm, file.path(prod_dir, paste0("dsm", res_txt, "_holes.tif")), overwrite = TRUE)

```

The generated DSM has many holes in it. This is overcome by filling those holes (NA values) with mean values from surrounding cells in a 3x3 moving window. This action is completed using the `terra` package since it is much faster than the `raster` package. Additionally, triangulation created odd edge effects at the map boundaries, so those will get erased by masking with the DEM which does not have those edge effects.

```{r DSM post processing}

# Load the DEM and DSM in terra
dem <- rast(file.path(prod_dir, paste0("dem", res_txt, ".tif")))
dsm <- rast(file.path(prod_dir, paste0("dsm", res_txt, "_holes.tif")))

# Use focal function to fill NA values in the DSM
dsm_filled <- focal(dsm, fun = "mean", na.only = TRUE)

# Match extents (crop to smallest one, this may not be the best but it works so far)
dem_area <- expanse(as.polygons(ext(dem), crs = crs(dem)))
dsm_area <- expanse(as.polygons(ext(dsm_filled), crs = crs(dsm_filled)))
if(dem_area > dsm_area) {
  dem <- crop(dem, dsm_filled)
  dsm <- dsm_filled
} else if(dem_area < dsm_area) {
  dsm <- crop(dsm_filled, dem)
} else {
  dsm <- dsm_filled
}

# Mask out the edges using the DEM, save the file on creation
dsm_mask <- mask(dsm, ctg_shp, filename = file.path(
  prod_dir, paste0("dsm", res_txt, ".tif")), overwrite = TRUE)

```

That's it! Below is some masked code for generating normalized tiles for developing a CHM, though this is not required for this project. It follows the same principles as DSM generation, just using normalized tiles:

1. Normalize the LAS tiles, save as new tiles
2. Use triangulation to create the CHM (same method as DSM generation, but also have some masked code in case that turns out to not be appropriate)
3. Fill NA values with mean of surrounding area.

```{r Normalize LAS tiles}

# Set catalog options
ctg <- readUAVLAScatalog(tile_dir)
opt_chunk_buffer(ctg) <- 12.5
opt_output_files(ctg) <- file.path(norm_dir, "{*}")
ctg@output_options$drivers$LAS$param$index <- TRUE

# Generate normalized tiles
set_lidr_threads(4)
plan(multisession(workers = 3))
ctg_norm <- normalize_height(ctg, algorithm = tin(), Wdegenerated = FALSE)

```

Now, create CHM. We can filter out points below 0 on the data load as these are errors. The CHM generated here uses the same ideas as the DSM generation, it's just performed on normalized tiles; however, code is provided to use the `pitfree()` algorithm as well. These should be compared, but it will take a long time to do so.

```{r CHM generation}

# Read the normalized tiles, drop points below 0
ctg_norm <- readUAVLAScatalog(norm_dir, filter = "-drop_z_below 0")

# Define which algorithm to use: pitfree or dsmtin
if(chm_fast) {
  algo <- dsmtin()
} else {
  max_z <- mean(ctg_norm$Max.Z)
  sq <- c(0, 2, seq(5, plyr::round_any(max_z, 5, ceiling), 5))
  max_edge <- c(0, min(map_res * 10, 1))
  subc <- map_res / 2
  algo <- pitfree(thresholds = sq, max_edge = max_edge, subcircle = subc)
}

# Set catalog options
opt_chunk_buffer(ctg_norm) <- 12.5
opt_output_files(ctg_norm) <- file.path(chm_dir, "{*}_chm")
ctg_norm@output_options$drivers$Raster$param$overwrite <- TRUE

# Generate CHM
set_lidr_threads(4)
plan(multisession(workers = 3))
chm <- grid_canopy(ctg_norm, res = map_res, algorithm = algo)
plan(sequential)
writeRaster(chm, file.path(prod_dir, paste0("chm", res_txt, "_holes.tif")), overwrite = TRUE)

```

Run the fill.na function over the CHM with holes.

```{r CHM post processing}

# Load the DEM and CHM in terra
dem <- rast(file.path(prod_dir, paste0("dem", res_txt, ".tif")))
chm <- rast(file.path(prod_dir, paste0("chm", res_txt, "_holes.tif")))

# Use focal function to fill NA values in the CHM, same function as with the DSM
chm_filled <- focal(chm, fun = "mean", na.only = TRUE)

# Match extents (crop to smallest one, this may not be the best but it works so far)
dem_area <- expanse(as.polygons(ext(dem), crs = crs(dem)))
chm_area <- expanse(as.polygons(ext(chm_filled), crs = crs(chm_filled)))
if(dem_area > chm_area) {
  dem <- crop(dem, chm_filled)
  chm <- chm_filled
} else if(dem_area < chm_area) {
  chm <- crop(chm_filled, dem)
} else {
  chm <- chm_filled
}

# Mask out the edges using the DEM, save the file on creation
chm_mask <- mask(chm, ctg_shp, filename = file.path(
  prod_dir, paste0("chm", res_txt, ".tif")), overwrite = TRUE)

```
